"""
Data transformation logic for ETL pipeline
"""

import logging
import pandas as pd
import numpy as np
from typing import List, Dict, Any
from datetime import datetime
from airflow.exceptions import AirflowException

logger = logging.getLogger(__name__)


def transform_data(
    data_source: str,
    transformations: List[str],
    data: List[Dict] = None
) -> List[Dict]:
    """
    Apply series of transformations to data
    
    Args:
        data_source: Source identifier ('api' or 'database')
        transformations: List of transformation names to apply
        data: Input data (optional - for testing)
        
    Returns:
        Transformed data as list of dictionaries
    """
    try:
        logger.info(f"Starting transformations for {data_source}: {transformations}")
        
        # Initialize with sample data if not provided
        if data is None:
            data = _get_sample_data(data_source)
        
        # Convert to DataFrame for easier manipulation
        df = pd.DataFrame(data)
        
        # Apply transformations in sequence
        for transformation in transformations:
            if transformation == 'clean':
                df = _clean_data(df, data_source)
            elif transformation == 'deduplicate':
                df = _deduplicate(df)
            elif transformation == 'validate':
                df = _validate_data(df)
            elif transformation == 'enrich':
                df = _enrich_data(df, data_source)
            else:
                logger.warning(f"Unknown transformation: {transformation}")
        
        # Convert back to list of dictionaries
        result = df.to_dict('records')
        logger.info(f"✓ Transformations complete. Output: {len(result)} records")
        
        return result
        
    except Exception as e:
        raise AirflowException(f"Transformation failed: {str(e)}")


def _clean_data(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """
    Clean and standardize data
    
    Operations:
    - Remove leading/trailing whitespace
    - Standardize data types
    - Handle missing values
    - Remove special characters from strings
    """
    logger.info("Cleaning data...")
    
    try:
        # Fill missing values
        df = df.fillna({
            'numeric_columns': 0,
            'string_columns': 'UNKNOWN'
        })
        
        # Trim whitespace from string columns
        string_cols = df.select_dtypes(include=['object']).columns
        for col in string_cols:
            df[col] = df[col].str.strip()
            df[col] = df[col].str.replace(r'[^\w\s\-\.@]', '', regex=True)
        
        # Convert date strings to datetime
        potential_date_cols = [col for col in df.columns 
                              if 'date' in col.lower() or 'time' in col.lower()]
        for col in potential_date_cols:
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
            except:
                pass
        
        # Convert numeric-looking strings to numeric
        numeric_cols = df.select_dtypes(include=['object']).columns
        for col in numeric_cols:
            try:
                df[col] = pd.to_numeric(df[col], errors='ignore')
            except:
                pass
        
        # Remove rows with all NaN values
        df = df.dropna(how='all')
        
        logger.info(f"✓ Cleaned {len(df)} records")
        return df
        
    except Exception as e:
        logger.error(f"Error cleaning data: {str(e)}")
        raise


def _deduplicate(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove duplicate records
    
    Operations:
    - Identify and remove exact duplicates
    - Handle fuzzy duplicates if applicable
    """
    logger.info("Deduplicating data...")
    
    try:
        initial_count = len(df)
        
        # Remove exact duplicates (keep first occurrence)
        df = df.drop_duplicates(keep='first', ignore_index=True)
        
        duplicates_removed = initial_count - len(df)
        logger.info(f"✓ Removed {duplicates_removed} duplicate records")
        
        return df
        
    except Exception as e:
        logger.error(f"Error deduplicating data: {str(e)}")
        raise


def _validate_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Validate data quality and consistency
    
    Operations:
    - Check required fields
    - Validate data types
    - Check business rules
    - Remove invalid records
    """
    logger.info("Validating data...")
    
    try:
        initial_count = len(df)
        
        # Define validation rules
        validation_rules = {
            'id_not_null': df['id'].notna() if 'id' in df.columns else pd.Series([True] * len(df)),
            'date_valid': df['created_at'].notna() if 'created_at' in df.columns else pd.Series([True] * len(df)),
        }
        
        # Apply validation rules
        valid_mask = pd.Series([True] * len(df))
        for rule_name, rule in validation_rules.items():
            valid_mask = valid_mask & rule
            invalid_count = (~rule).sum()
            if invalid_count > 0:
                logger.warning(f"Rule '{rule_name}' failed for {invalid_count} records")
        
        # Keep only valid records
        df = df[valid_mask].reset_index(drop=True)
        
        invalid_removed = initial_count - len(df)
        logger.info(f"✓ Validation complete. Removed {invalid_removed} invalid records")
        
        return df
        
    except Exception as e:
        logger.error(f"Error validating data: {str(e)}")
        raise


def _enrich_data(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """
    Enrich data with additional computed fields
    
    Operations:
    - Add calculated columns
    - Add metadata
    - Add source information
    """
    logger.info("Enriching data...")
    
    try:
        # Add source column
        df['data_source'] = source
        
        # Add processing timestamp
        df['processed_at'] = datetime.utcnow()
        
        # Add hash for row versioning
        if 'id' in df.columns:
            df['row_hash'] = df.astype(str).apply(
                lambda row: hash(tuple(row)) % ((2**63) - 1),
                axis=1
            )
        
        logger.info("✓ Data enriched with metadata")
        return df
        
    except Exception as e:
        logger.error(f"Error enriching data: {str(e)}")
        raise


def _get_sample_data(source: str) -> List[Dict]:
    """Generate sample data for testing"""
    
    if source == 'api':
        return [
            {
                'id': 1,
                'name': 'Job Listing 1',
                'title': 'Senior Data Engineer',
                'company': 'Tech Corp',
                'salary': '120000',
                'created_at': '2024-01-01',
                'location': 'San Francisco, CA'
            },
            {
                'id': 2,
                'name': 'Job Listing 2',
                'title': 'Data Analyst',
                'company': 'Data Solutions Inc',
                'salary': '85000',
                'created_at': '2024-01-02',
                'location': 'Remote'
            }
        ]
    else:  # database
        return [
            {
                'id': 101,
                'user_id': 1,
                'action': 'applied',
                'job_id': 1,
                'timestamp': '2024-01-01 10:00:00',
                'status': 'pending',
                'updated_at': '2024-01-01'
            },
            {
                'id': 102,
                'user_id': 2,
                'action': 'viewed',
                'job_id': 2,
                'timestamp': '2024-01-02 14:30:00',
                'status': 'active',
                'updated_at': '2024-01-02'
            }
        ]


def validate_data(
    total_records: int,
    quality_threshold: float = 0.95
) -> Dict[str, Any]:
    """
    Validate merged data quality
    
    Args:
        total_records: Total records in dataset
        quality_threshold: Minimum acceptable quality score (0-1)
        
    Returns:
        Validation results with quality score
    """
    try:
        # Simulate quality checks
        # In production, these would check actual data quality metrics
        
        checks = {
            'completeness': 0.98,  # % of non-null values
            'accuracy': 0.96,      # % of valid values
            'consistency': 0.99,   # % of consistent values
            'uniqueness': 0.97,    # % of unique values where expected
        }
        
        # Calculate overall quality score
        quality_score = np.mean(list(checks.values()))
        
        results = {
            'is_valid': quality_score >= quality_threshold,
            'quality_score': quality_score,
            'checks': checks,
            'total_records': total_records,
            'issues': []
        }
        
        # Identify issues
        for check_name, check_value in checks.items():
            if check_value < quality_threshold:
                results['issues'].append(
                    f"{check_name} below threshold: {check_value:.2%}"
                )
        
        return results
        
    except Exception as e:
        raise AirflowException(f"Data validation failed: {str(e)}")
